{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf4e37fa-8d59-4106-92d1-8f956647aba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/ben/miniconda3/lib/python3.10/site-packages (3.7.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: numpy>=1.20 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (9.5.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ben/miniconda3/lib/python3.10/site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ben/miniconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ben/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch.nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "0161596a-a60d-4d0f-87a2-f9d9e1f656d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tensor(tensor):\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(tensor, cmap='viridis') # You can use other colormaps like 'plasma', 'inferno', 'magma', etc.\n",
    "    plt.show()\n",
    "\n",
    "class Register(object):\n",
    "    def __init__(self, name, size):\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self.offset = None\n",
    "\n",
    "class Embedding(object):\n",
    "    def __init__(self, tokens: list[str], registers: list[Register]):\n",
    "        self.tokens = tokens\n",
    "        self.token_map = { t: i for i,t in enumerate(tokens) }\n",
    "        self.registers = registers\n",
    "        self.register_map = {}\n",
    "        self.register_size = 0\n",
    "        \n",
    "        if len(registers) == 0 or registers[0].name != 'pos':\n",
    "            raise Exception(\"First register must be 'pos'\") \n",
    "        \n",
    "        offset = len(tokens)\n",
    "        for reg in registers:\n",
    "            reg.offset = offset\n",
    "            offset += reg.size\n",
    "            self.register_size += reg.size\n",
    "            self.register_map[reg.name] = reg\n",
    "            \n",
    "        self.dim = len(tokens) + self.register_size\n",
    "\n",
    "    def tokenize(self, string: str):\n",
    "        return F.one_hot(torch.tensor([self.token_map[c] for c in string]), num_classes=len(self.tokens)).float()\n",
    "\n",
    "    def embed(self, sequence):\n",
    "        # We want to create additional space to store the registers\n",
    "        extension_tensor = torch.zeros(*sequence.shape[:-1], self.register_size)\n",
    "\n",
    "        # Encode position in the first extra embedding dimension\n",
    "        for i in range(sequence.shape[0]):\n",
    "            extension_tensor[i, 0] = math.sin(i*(2*math.pi)/100)\n",
    "            extension_tensor[i, 1] = math.cos(i*(2*math.pi)/100)\n",
    "\n",
    "        sequence = torch.cat((sequence, extension_tensor), dim=-1)\n",
    "\n",
    "        return sequence\n",
    "    \n",
    "    def predict(self, sequence):\n",
    "        return self.tokens[torch.argmax(sequence[-1,:])]\n",
    "\n",
    "class AttentionLayer(object):\n",
    "    def __init__(self, instruction):\n",
    "        self.instruction = instruction\n",
    "        \n",
    "    def attend(self, seq):\n",
    "        query = seq @ self.instruction.query\n",
    "        key = seq @ self.instruction.key\n",
    "        value = seq @ self.instruction.value\n",
    "\n",
    "        causal_mask = torch.triu(torch.ones(seq.shape[0], seq.shape[0]), diagonal=1)*-1e10\n",
    "        norm = np.sqrt(seq.shape[-1])\n",
    "        \n",
    "        kq = torch.nn.Softmax(1)(query @ key.T / norm + causal_mask)\n",
    "        #print('attention query')\n",
    "        #plot_tensor(query)\n",
    "        #print('key')\n",
    "        #plot_tensor(key)\n",
    "        #plot_tensor(query @ key.T)\n",
    "        #plot_tensor(kq)\n",
    "        #print(query @ key.T)\n",
    "                    \n",
    "        s = kq @ value\n",
    "        #plot_tensor(s)\n",
    "        return (seq + s)\n",
    "    \n",
    "class MLPLayer(object):\n",
    "    def __init__(self, instruction, debug=False):\n",
    "        self.instruction = instruction\n",
    "        self.debug = debug\n",
    "        \n",
    "    def forward(self, seq):\n",
    "        a = torch.nn.GELU()(seq @ self.instruction.first_weights + self.instruction.first_bias)\n",
    "        if self.debug:\n",
    "            plot_tensor(a)\n",
    "            print('AAA', a)\n",
    "        b = (a @ self.instruction.second_weights)\n",
    "        if self.debug:\n",
    "            plot_tensor(b)\n",
    "            print('BBB', b)\n",
    "        x = b + self.instruction.second_bias\n",
    "        if self.debug:\n",
    "            plot_tensor(x)\n",
    "            print('XXX',x)\n",
    "        return seq + x\n",
    "    \n",
    "tokens = list('0123456789+= \\n')\n",
    "pos = Register('pos', 2)\n",
    "left_pos = Register('left_pos', 2)\n",
    "right_pos = Register('right_pos', 2)\n",
    "out_pos = Register('out_pos', 2)\n",
    "left_digit = Register('left', len(tokens))\n",
    "right_digit = Register('right', len(tokens))\n",
    "out_digit = Register('out', len(tokens))\n",
    "final_digit = Register('final', len(tokens))\n",
    "carry = Register('carry', 1)\n",
    "distance = Register('distance', 1)\n",
    "\n",
    "embedding = Embedding(tokens, [pos, left_pos, right_pos, out_pos, left_digit, right_digit, out_digit, carry, distance, final_digit])\n",
    "        \n",
    "class FindAndStore(object):\n",
    "    def __init__(self, embedding: Embedding, token: str, register: Register):\n",
    "        pos_reg = embedding.register_map['pos']\n",
    "        \n",
    "        # No matter the current token, we attend to if the attended token is the given token\n",
    "        token_select = torch.zeros(embedding.dim, embedding.dim) - 1e10\n",
    "        token_select[:, int(embedding.token_map[token])] = 1e10\n",
    "\n",
    "        position_select = torch.zeros(embedding.dim, embedding.dim)\n",
    "        position_select[pos_reg.offset, register.offset] = 1.0\n",
    "        position_select[pos_reg.offset + 1, register.offset + 1] = 1.0\n",
    "        \n",
    "        self.key = torch.eye(embedding.dim)\n",
    "        self.query = token_select\n",
    "        self.value = position_select\n",
    "\n",
    "ex = embedding.embed(embedding.tokenize('10+10=2111')) \n",
    "        \n",
    "class GetRelativeToken(object):\n",
    "    def __init__(self, embedding: Embedding, pos_reg: Register, steps: int, out: Register):\n",
    "        tpos_reg = embedding.register_map['pos']\n",
    "        \n",
    "        position_select = torch.zeros(embedding.dim, embedding.dim)\n",
    "        position_select[tpos_reg.offset, tpos_reg.offset] = 1e10\n",
    "        position_select[tpos_reg.offset + 1, tpos_reg.offset + 1] = 1e10\n",
    "\n",
    "        i = -steps\n",
    "        sin = math.sin(i*(2*math.pi)/100)*1\n",
    "        cos = math.cos(i*(2*math.pi)/100)*1\n",
    "\n",
    "        rotation = torch.zeros(embedding.dim, embedding.dim)\n",
    "        rotation[pos_reg.offset, tpos_reg.offset] = cos\n",
    "        rotation[pos_reg.offset + 1, tpos_reg.offset] = -sin\n",
    "        rotation[pos_reg.offset, tpos_reg.offset + 1] = sin\n",
    "        rotation[pos_reg.offset + 1, tpos_reg.offset + 1] = cos\n",
    "        #plot_tensor(rotation)\n",
    "        \n",
    "        token_copy = torch.zeros(embedding.dim, embedding.dim)\n",
    "        for i in range(len(embedding.tokens)):\n",
    "            token_copy[i, i + out.offset] = 1.0\n",
    "            \n",
    "        self.query = rotation\n",
    "        self.key = position_select\n",
    "        self.value = token_copy\n",
    "        \n",
    "class Multiply(object):\n",
    "    def __init__(self, embedding: Embedding, left_token: Register, right_token: Register, carry: Register, out_token: Register):\n",
    "        width = 10*10*2\n",
    "\n",
    "        self.first_weights = torch.zeros(embedding.dim, width)\n",
    "        self.first_bias = torch.zeros(width)\n",
    "        \n",
    "        self.second_weights = torch.zeros(width, embedding.dim)\n",
    "        self.second_bias = torch.zeros(embedding.dim)\n",
    "\n",
    "        idx = 0\n",
    "        for i in range(0, 10):\n",
    "            for j in range(0, 10):\n",
    "                self.first_weights[left_token.offset + i, idx] = 500\n",
    "                self.first_weights[right_token.offset + j, idx] = 500\n",
    "                self.first_weights[carry.offset, idx] = -1000\n",
    "                self.first_bias[idx] = -900\n",
    "                self.second_weights[idx, out_token.offset + (i + j) % 10] = 0.01\n",
    "                if (i + j) >= 10:\n",
    "                    self.second_weights[idx, carry.offset] = 0.01\n",
    "                idx += 1\n",
    "                \n",
    "                self.first_weights[left_token.offset + i, idx] = 333\n",
    "                self.first_weights[right_token.offset + j, idx] = 333\n",
    "                self.first_weights[carry.offset, idx] = 333\n",
    "                self.first_bias[idx] = -900\n",
    "                self.second_weights[idx, out_token.offset + (i + j + 1) % 10] = 0.01 * (1.0/0.99)\n",
    "                # If we need to carry, there was already a carry so we change nothing \n",
    "                if (i + j + 1) >= 10:\n",
    "                    self.second_weights[idx, carry.offset] = 0.0\n",
    "                # If we don't need to carry, we need to clear the carry bit,\n",
    "                else:\n",
    "                    self.second_weights[idx, carry.offset] = -0.01 * (1.0/0.99)\n",
    "                idx += 1\n",
    "                \n",
    "class Clear(object):\n",
    "    def __init__(self, embedding: Embedding, registers: list[Register]):\n",
    "        self.first_weights = torch.zeros(embedding.dim, embedding.dim)\n",
    "        self.first_bias = torch.zeros(embedding.dim)\n",
    "        \n",
    "        for reg in registers:\n",
    "            for i in range(reg.size):\n",
    "                self.first_weights[reg.offset + i, reg.offset + i] = 100.0\n",
    "        \n",
    "        self.second_weights = torch.zeros(embedding.dim, embedding.dim)\n",
    "        self.second_bias = torch.zeros(embedding.dim)\n",
    "        for reg in registers:\n",
    "            for i in range(reg.size):\n",
    "                self.second_weights[reg.offset + i, reg.offset + i] = -0.01\n",
    "                \n",
    "class DiffPos(object):\n",
    "    def __init__(self, embedding: Embedding, left_pos: Register, right_pos: Register, distance: Register):\n",
    "        self.first_weights = torch.zeros(embedding.dim, 2)\n",
    "        self.first_bias = torch.zeros(2)\n",
    "        \n",
    "        # Note: it's important that the x and y are multiplied by different numbers,\n",
    "        # otherwise 1, 2 computes to be the same as 2, 1\n",
    "        \n",
    "        self.first_weights[left_pos.offset, 0] = 1e2\n",
    "        self.first_weights[left_pos.offset + 1, 0] = 1e3\n",
    "        self.first_weights[right_pos.offset, 0] = -1e2\n",
    "        self.first_weights[right_pos.offset + 1, 0] = -1e3\n",
    "        \n",
    "        self.first_weights[left_pos.offset, 1] = -1e2\n",
    "        self.first_weights[left_pos.offset + 1, 1] = -1e3\n",
    "        self.first_weights[right_pos.offset, 1] = 1e2\n",
    "        self.first_weights[right_pos.offset + 1, 1] = 1e3\n",
    "        \n",
    "        self.second_weights = torch.zeros(2, embedding.dim)\n",
    "        self.second_bias = torch.zeros(embedding.dim)\n",
    "        \n",
    "        self.second_weights[0, distance.offset] = 1.0\n",
    "        self.second_weights[1, distance.offset] = 1.0\n",
    "        \n",
    "class IsZero(object):\n",
    "    def __init__(self, embedding: Embedding, zero: Register):\n",
    "        self.first_weights = torch.zeros(embedding.dim, 2)\n",
    "        self.first_bias = torch.zeros(1)\n",
    "        \n",
    "        self.first_weights[zero.offset, 0] = -100.0\n",
    "        self.first_weights[zero.offset, 1] = 1.0\n",
    "        self.first_bias[0] = 10.0\n",
    "        \n",
    "        self.second_weights = torch.zeros(2, embedding.dim)\n",
    "        self.second_bias = torch.zeros(embedding.dim)\n",
    "        \n",
    "        self.second_weights[0, zero.offset] = 0.1\n",
    "        self.second_weights[1, zero.offset] = -1.0\n",
    "        self.second_bias[zero.offset] = 10.0\n",
    "        \n",
    "class CopyOnMatch(object):\n",
    "    def __init__(self, embedding: Embedding, match: Register, src: Register, dst: Register):\n",
    "        self.first_weights = torch.zeros(embedding.dim, src.size)\n",
    "        self.first_bias = torch.ones(src.size)\n",
    "        \n",
    "        for i in range(src.size):\n",
    "            self.first_weights[match.offset, i] = 100\n",
    "            self.first_weights[src.offset + i, i] = 100\n",
    "            self.first_bias[i] = -190\n",
    "        \n",
    "        self.second_weights = torch.zeros(src.size, embedding.dim)\n",
    "        self.second_bias = torch.zeros(embedding.dim)\n",
    "        \n",
    "        for i in range(src.size):\n",
    "            self.second_weights[i, dst.offset + i] = 0.1\n",
    "\n",
    "class StepPosition(object):\n",
    "    def __init__(self, embedding: Embedding, positions: list[Register], offsets: list[Register]):\n",
    "\n",
    "        rotation = torch.zeros(embedding.dim, embedding.dim)\n",
    "        \n",
    "        n = 0\n",
    "        for reg in positions:\n",
    "            i = offsets[n]\n",
    "            sin = math.sin(-i*(2*math.pi)/100)*1\n",
    "            cos = math.cos(-i*(2*math.pi)/100)*1\n",
    "        \n",
    "            rotation[reg.offset, reg.offset] = cos - 1\n",
    "            rotation[reg.offset + 1, reg.offset] = -sin\n",
    "            rotation[reg.offset, reg.offset + 1] = sin\n",
    "            rotation[reg.offset + 1, reg.offset + 1] = cos - 1\n",
    "            \n",
    "            n += 1\n",
    "\n",
    "        self.first_weights = rotation\n",
    "        self.first_bias = torch.ones(embedding.dim)*1000\n",
    "        \n",
    "        self.second_weights = torch.eye(embedding.dim, embedding.dim)\n",
    "        self.second_bias = torch.ones(embedding.dim)*-1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "d9700648-a51f-4123-a487-bd8f6dc2ce8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0999+0111=0\n",
      "0999+0111=01\n",
      "0999+0111=011\n",
      "0999+0111=0111\n",
      "0999+0111=01110\n",
      "0999+0111=011100\n",
      "0999+0111=0111000\n",
      "0999+0111=01110000\n",
      "0999+0111=011100000\n",
      "0999+0111=0111000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 1000000/1000000 [3:33:21<00:00, 78.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All done! Passed count:  1000000 of 1000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "# Attention layer to search for the location of various tokens\n",
    "find_plus = AttentionLayer(FindAndStore(embedding, '+', left_pos))\n",
    "find_equal = AttentionLayer(FindAndStore(embedding, '=', right_pos))\n",
    "find_out = AttentionLayer(FindAndStore(embedding, '=', out_pos))\n",
    "\n",
    "# Fully connected layers to adjust positions by fixed amounts\n",
    "step_out = MLPLayer(StepPosition(embedding, [out_pos], [-1]))\n",
    "step_pos = MLPLayer(StepPosition(embedding, [left_pos, right_pos, out_pos], [-1, -1, 1]))\n",
    "\n",
    "# Layers to read the token at a specific position\n",
    "read_left = AttentionLayer(GetRelativeToken(embedding, left_pos, 0, left_digit))\n",
    "read_right = AttentionLayer(GetRelativeToken(embedding, right_pos, 0, right_digit))\n",
    "\n",
    "# Layers to look up how tokens combine (symbolically)\n",
    "multiply = MLPLayer(Multiply(embedding, left_digit, right_digit, carry, out_digit))\n",
    "\n",
    "# Utility layer to clear the residual stream\n",
    "clear_lro = MLPLayer(Clear(embedding, [left_digit, right_digit, out_digit]))\n",
    "\n",
    "# Layers to keep track of if the out cursor is the next token and if so copy the out digit\n",
    "diff_pos = MLPLayer(DiffPos(embedding, pos, out_pos, distance))\n",
    "is_zero = MLPLayer(IsZero(embedding, distance))\n",
    "copy_out = MLPLayer(CopyOnMatch(embedding, distance, out_digit, final_digit))\n",
    "\n",
    "# Linear layer to move the final digit to the out digit\n",
    "final_projection = torch.zeros(embedding.dim, embedding.dim)\n",
    "for i in range(final_digit.size):\n",
    "    final_projection[final_digit.offset + i, i] = 1.0\n",
    "\n",
    "def read_out(x):\n",
    "    print('Out:', torch.argmax(x[-1,out_digit.offset:out_digit.offset + out_digit.size]))\n",
    "\n",
    "def generate(input_string):\n",
    "    \n",
    "    # First we embed the original string\n",
    "    x = embedding.embed(embedding.tokenize(input_string))\n",
    "\n",
    "    # Then we look for the various symbols that direct us to\n",
    "    x = find_plus.attend(x)\n",
    "    x = find_equal.attend(x)\n",
    "    x = find_out.attend(x)\n",
    "    x = step_out.forward(x)\n",
    "\n",
    "    for i in range(4):\n",
    "\n",
    "        x = clear_lro.forward(x)\n",
    "        x = step_pos.forward(x)\n",
    "        x = read_left.attend(x)\n",
    "        x = read_right.attend(x)\n",
    "        x = multiply.forward(x)\n",
    "\n",
    "        # If the focused output is this one, copy the output\n",
    "        x = diff_pos.forward(x)\n",
    "        #print(x[:, out_pos.offset:out_pos.offset + 2])\n",
    "        #print(x[:, distance.offset])\n",
    "        x = is_zero.forward(x)\n",
    "        x = copy_out.forward(x)\n",
    "\n",
    "        #plot_tensor(x)\n",
    "        #read_out(x)\n",
    "\n",
    "    #plot_tensor(x)\n",
    "    x =  x @ final_projection\n",
    "    #plot_tensor(x)\n",
    "\n",
    "    return input_string + embedding.predict(x)\n",
    "\n",
    "test = '0999+0111='\n",
    "for i in range(10):\n",
    "    test = generate(test)\n",
    "    print(test)\n",
    "\n",
    "to_check = [(i, j) for i in range(1000) for j in range(1000)]\n",
    "passed = 0\n",
    "\n",
    "for i, j in tqdm.tqdm(to_check):\n",
    "    test = str(i).zfill(4) + '+' + str(j).zfill(4) + '='\n",
    "    expected = test + str(i + j).zfill(4)[::-1]\n",
    "\n",
    "    for n in range(4):\n",
    "        test = generate(test)\n",
    "\n",
    "    if test != expected:\n",
    "        print(\"Failed!\", test, expected)\n",
    "    else:\n",
    "        passed += 1\n",
    "\n",
    "print(\"All done! Passed count: \", passed, \"of\", len(to_check))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca04feba-b8b6-445e-ad3e-7aa9bda9f5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
